{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbae7e72",
   "metadata": {},
   "source": [
    "\n",
    "# 🧪 Deepfake Detection — Jupyter Notebook (Keras + Gradio)\n",
    "\n",
    "This notebook lets you **train** and **demo** a Deepfake Detector end‑to‑end:\n",
    "\n",
    "- ✅ Dataset loaders (train/val split via `flow_from_directory`)\n",
    "- ✅ Choice of **Simple CNN** or **EfficientNetB0** (transfer learning)\n",
    "- ✅ Saves `models/deepfake_detector_keras.h5` and `models/labels.json`\n",
    "- ✅ Quick evaluation & **threshold** suggestion\n",
    "- ✅ **Gradio UI** for uploads and **webcam** (if supported)\n",
    "- ✅ **Mock Mode** fallback if no model is loaded\n",
    "\n",
    "> **Dataset format (binary classes):**\n",
    ">\n",
    "> ```\n",
    "> data/dataset/\n",
    "> ├── real/\n",
    "> └── fake/\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87870e72",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8cfaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.19.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If needed, install packages (uncomment as necessary)\n",
    "# %pip install tensorflow==2.15.0 pillow numpy opencv-python-headless gradio==4.44.0 scikit-learn\n",
    "\n",
    "import os, json, math, time, pathlib, typing, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fff575",
   "metadata": {},
   "source": [
    "## 2) Configure paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc497dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: d:\\Mtech\\Research paper\\DeepFake Detection\\Minor Version\n",
      "DATA_DIR exists: True\n",
      "MODEL_DIR: d:\\Mtech\\Research paper\\DeepFake Detection\\Minor Version\\models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Change these if your structure differs\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"dataset\"   # expects subfolders: real/, fake/\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = MODEL_DIR / \"deepfake_detector_keras.h5\"\n",
    "LABELS_PATH = MODEL_DIR / \"labels.json\"\n",
    "\n",
    "IMG_SIZE = (128, 128)   # can override to (224,224) for EfficientNet\n",
    "BATCH = 64\n",
    "VAL_SPLIT = 0.2\n",
    "EPOCHS = 12          # start with 8–15, tune later\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATA_DIR exists:\", DATA_DIR.exists())\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a612286",
   "metadata": {},
   "source": [
    "## 3) Data generators (train/val split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deed1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16002 images belonging to 2 classes.\n",
      "Found 4000 images belonging to 2 classes.\n",
      "Saved class indices: {'Fake': 0, 'Real': 1}\n",
      "labels.json: d:\\Mtech\\Research paper\\DeepFake Detection\\Minor Version\\models\\labels.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise SystemExit(f\"Dataset folder not found: {DATA_DIR}. Put images under real/ and fake/ subfolders.\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255.0,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.02,\n",
    "    height_shift_range=0.02,\n",
    "    zoom_range=0.5,\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    DATA_DIR.as_posix(),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    DATA_DIR.as_posix(),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Save class indices for later (UI needs to know which class is index 1)\n",
    "with open(LABELS_PATH, \"w\") as f:\n",
    "    json.dump(train_gen.class_indices, f)\n",
    "print(\"Saved class indices:\", train_gen.class_indices)\n",
    "print(\"labels.json:\", LABELS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea13185",
   "metadata": {},
   "source": [
    "## 4) Choose model: Simple CNN or EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a71f24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,569</span> (396.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,569\u001b[0m (396.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,569</span> (396.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,569\u001b[0m (396.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "USE_EFFICIENTNET = False   # <-- flip to True for transfer learning\n",
    "\n",
    "def build_simple_cnn(input_shape=(128,128,3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_efficientnet_b0(input_shape=(224,224,3)):\n",
    "    from tensorflow.keras.applications import EfficientNetB0\n",
    "    from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "    base = EfficientNetB0(include_top=False, input_shape=input_shape, weights=\"imagenet\")\n",
    "    base.trainable = False  # freeze first stage\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = preprocess_input(inputs)\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.35)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# pick model\n",
    "if USE_EFFICIENTNET:\n",
    "    IMG_SIZE = (224,224)\n",
    "    # re-build generators with new image size\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        DATA_DIR.as_posix(),\n",
    "        target_size=IMG_SIZE, batch_size=BATCH, class_mode=\"binary\", subset=\"training\", shuffle=True\n",
    "    )\n",
    "    val_gen = train_datagen.flow_from_directory(\n",
    "        DATA_DIR.as_posix(),\n",
    "        target_size=IMG_SIZE, batch_size=BATCH, class_mode=\"binary\", subset=\"validation\", shuffle=False\n",
    "    )\n",
    "    model = build_efficientnet_b0(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "else:\n",
    "    model = build_simple_cnn(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f020cb2",
   "metadata": {},
   "source": [
    "## 5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9880549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.5282 - loss: 0.6903"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 461ms/step - accuracy: 0.5427 - loss: 0.6878 - val_accuracy: 0.6292 - val_loss: 0.6666\n",
      "Epoch 2/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376ms/step - accuracy: 0.5631 - loss: 0.6820"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 445ms/step - accuracy: 0.5652 - loss: 0.6817 - val_accuracy: 0.6585 - val_loss: 0.6505\n",
      "Epoch 3/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - accuracy: 0.5703 - loss: 0.6770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 466ms/step - accuracy: 0.5730 - loss: 0.6785 - val_accuracy: 0.6607 - val_loss: 0.6453\n",
      "Epoch 4/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 456ms/step - accuracy: 0.5772 - loss: 0.6766 - val_accuracy: 0.5957 - val_loss: 0.6687\n",
      "Epoch 5/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - accuracy: 0.5734 - loss: 0.6766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 592ms/step - accuracy: 0.5799 - loss: 0.6737 - val_accuracy: 0.6647 - val_loss: 0.6408\n",
      "Epoch 6/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - accuracy: 0.5986 - loss: 0.6657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 615ms/step - accuracy: 0.6032 - loss: 0.6640 - val_accuracy: 0.6930 - val_loss: 0.6099\n",
      "Epoch 7/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534ms/step - accuracy: 0.6205 - loss: 0.6531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 639ms/step - accuracy: 0.6130 - loss: 0.6566 - val_accuracy: 0.7163 - val_loss: 0.6008\n",
      "Epoch 8/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 666ms/step - accuracy: 0.6236 - loss: 0.6474 - val_accuracy: 0.6830 - val_loss: 0.5961\n",
      "Epoch 9/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536ms/step - accuracy: 0.6318 - loss: 0.6446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 645ms/step - accuracy: 0.6335 - loss: 0.6397 - val_accuracy: 0.7405 - val_loss: 0.5660\n",
      "Epoch 10/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 641ms/step - accuracy: 0.6509 - loss: 0.6273 - val_accuracy: 0.7360 - val_loss: 0.5654\n",
      "Epoch 11/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.6656 - loss: 0.6188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 659ms/step - accuracy: 0.6705 - loss: 0.6115 - val_accuracy: 0.7448 - val_loss: 0.5378\n",
      "Epoch 12/12\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 642ms/step - accuracy: 0.6844 - loss: 0.5962 - val_accuracy: 0.7335 - val_loss: 0.5388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: d:\\Mtech\\Research paper\\DeepFake Detection\\Minor Version\\models\\deepfake_detector_keras.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ckpt = callbacks.ModelCheckpoint(MODEL_PATH.as_posix(), monitor=\"val_accuracy\", save_best_only=True, mode=\"max\")\n",
    "es = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[ckpt, es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(MODEL_PATH.as_posix())\n",
    "print(\"Saved model to:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed3cc7",
   "metadata": {},
   "source": [
    "## 6) Quick evaluation & threshold suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb3e7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class index 1 is: Real\n",
      "Suggested threshold: 0.70 (F1=0.287)\n",
      "[[1398  602]\n",
      " [1934   66]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.42      0.70      0.52      2000\n",
      "        Real       0.10      0.03      0.05      2000\n",
      "\n",
      "    accuracy                           0.37      4000\n",
      "   macro avg       0.26      0.37      0.29      4000\n",
      "weighted avg       0.26      0.37      0.29      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "val_gen.reset()\n",
    "p1 = model.predict(val_gen, verbose=0).ravel()  # prob of class index 1\n",
    "y_true = val_gen.classes\n",
    "\n",
    "# Load mapping and infer which class is index 1\n",
    "with open(LABELS_PATH, \"r\") as f:\n",
    "    idx = json.load(f)  # e.g., {'fake': 0, 'real': 1}\n",
    "inv = {v:k for k,v in idx.items()}\n",
    "class1 = inv.get(1, None)\n",
    "print(\"Class index 1 is:\", class1)\n",
    "\n",
    "# p(fake) depends on what class 1 represents\n",
    "if class1 and class1.lower() == \"real\":\n",
    "    p_fake = 1.0 - p1\n",
    "else:\n",
    "    p_fake = p1  # class 1 is 'fake' or unknown mapping\n",
    "\n",
    "ths = np.linspace(0.3, 0.7, 41)\n",
    "best_f1, best_th = -1, 0.5\n",
    "for th in ths:\n",
    "    y_hat = (p_fake >= th).astype(int)\n",
    "    f1 = f1_score(y_true, y_hat, average=\"macro\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_th = f1, th\n",
    "\n",
    "print(f\"Suggested threshold: {best_th:.2f} (F1={best_f1:.3f})\")\n",
    "print(confusion_matrix(y_true, (p_fake >= best_th).astype(int)))\n",
    "print(classification_report(y_true, (p_fake >= best_th).astype(int), target_names=[inv.get(0,'class0'), inv.get(1,'class1')]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea219b0",
   "metadata": {},
   "source": [
    "## 7) Gradio demo (upload & webcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989bf1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\loq\\anaconda3\\lib\\site-packages (4.12.0.88)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio==4.44.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.29.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from gradio==4.44.0) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from gradio==4.44.0) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (3.9.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (2.2.6)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (3.11.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (2.8.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.12.8)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (4.11.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (2.5.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio==4.44.0) (0.35.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio-client==1.3.0->gradio==4.44.0) (2024.6.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gradio-client==1.3.0->gradio==4.44.0) (10.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio==4.44.0) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio==4.44.0) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from fastapi<1.0->gradio==4.44.0) (0.47.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\loq\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio==4.44.0) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\loq\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio==4.44.0) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.44.0) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio==4.44.0) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio==4.44.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio==4.44.0) (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==4.44.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==4.44.0) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio==4.44.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio==4.44.0) (2.20.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0,>=0.12->gradio==4.44.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio==4.44.0) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio==4.44.0) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio==4.44.0) (0.4.6)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio==4.44.0)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.44.0) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio==4.44.0) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (0.1.0)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "%pip install gradio==4.44.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fdf98b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gradio Blocks instance: 1 backend functions\n",
       "-------------------------------------------\n",
       "fn_index=0\n",
       " inputs:\n",
       " |-<gradio.components.image.Image object at 0x00000139376A9610>\n",
       " outputs:\n",
       " |-<gradio.components.textbox.Textbox object at 0x00000139376533E0>\n",
       " |-<gradio.components.slider.Slider object at 0x000001393773A630>\n",
       " |-<gradio.components.textbox.Textbox object at 0x0000013937706E70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import io, base64, cv2, gradio as gr\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "# Reload model (for safety if you restart kernel cells)\n",
    "try:\n",
    "    mdl = tf.keras.models.load_model(MODEL_PATH.as_posix())\n",
    "    with open(LABELS_PATH, \"r\") as f:\n",
    "        idx = json.load(f)\n",
    "    inv = {v:k for k,v in idx.items()}\n",
    "    class1 = inv.get(1, None)\n",
    "    model_loaded = True\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Failed to load model, using Mock Mode:\", e)\n",
    "    mdl = None\n",
    "    class1 = None\n",
    "    model_loaded = False\n",
    "\n",
    "THRESH = float(globals().get(\"best_th\", 0.5))  # pick the suggested threshold if available\n",
    "\n",
    "def preprocess_pil(img: Image.Image, size=(128,128)):\n",
    "    img = img.convert(\"RGB\").resize(size)\n",
    "    arr = np.asarray(img).astype(\"float32\") / 255.0\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    return arr\n",
    "\n",
    "def mock_fake_probability(img: Image.Image) -> float:\n",
    "    try:\n",
    "        img_cv = cv2.cvtColor(np.array(img.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "        var_lap = cv2.Laplacian(img_cv, cv2.CV_64F).var()\n",
    "    except Exception:\n",
    "        var_lap = 50.0\n",
    "    edges = img.convert(\"L\").filter(ImageFilter.FIND_EDGES)\n",
    "    edge_mean = np.array(edges).mean()\n",
    "    sharp = np.tanh(var_lap / 200.0)\n",
    "    edginess = np.tanh(edge_mean / 64.0)\n",
    "    score = 0.6 * (1 - sharp) + 0.4 * (1 - edginess)\n",
    "    return float(np.clip(score, 0, 1))\n",
    "\n",
    "def predict_image(pil_image):\n",
    "    if pil_image is None:\n",
    "        return \"No image\", 0.0, \"Provide an image.\"\n",
    "    if model_loaded and mdl is not None:\n",
    "        size = mdl.inputs[0].shape[1:3]\n",
    "        size = (int(size[0]), int(size[1]))\n",
    "        arr = preprocess_pil(pil_image, size=size)\n",
    "        p1 = float(mdl.predict(arr, verbose=0)[0][0])  # prob of class index 1\n",
    "        if class1 and class1.lower() == \"real\":\n",
    "            p_fake = 1.0 - p1\n",
    "        else:\n",
    "            p_fake = p1\n",
    "    else:\n",
    "        p_fake = mock_fake_probability(pil_image)\n",
    "\n",
    "    label = \"FAKE\" if p_fake >= THRESH else \"REAL\"\n",
    "    note = \"Using trained model\" if model_loaded else \"Using Mock Mode (demo heuristic)\"\n",
    "    return label, p_fake, note\n",
    "\n",
    "title = \"Deepfake Detector (Notebook Demo)\"\n",
    "desc = \"Upload an image or use webcam. The app returns a label and a fake probability. Shows demo heuristic if model isn't loaded.\"\n",
    "\n",
    "with gr.Blocks(title=title) as demo:\n",
    "    gr.Markdown(f\"\"\"### {title}\n",
    "{desc}\n",
    "\n",
    "**Model loaded:** {model_loaded}  \n",
    "**Threshold:** {THRESH:.2f}\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            img_in = gr.Image(type=\"pil\", label=\"Input image\", sources=[\"upload\", \"webcam\"])\n",
    "            btn = gr.Button(\"Analyze\")\n",
    "        with gr.Column():\n",
    "            out_label = gr.Textbox(label=\"Prediction\", interactive=False)\n",
    "            out_prob = gr.Slider(0, 1, value=0.0, step=0.001, label=\"Fake probability\", interactive=False)\n",
    "            out_note = gr.Textbox(label=\"Note\", interactive=False)\n",
    "    btn.click(fn=predict_image, inputs=img_in, outputs=[out_label, out_prob, out_note])\n",
    "\n",
    "demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef67847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== explain_utils.py =====================\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def grad_cam(model, img_bgr, target_size=(300, 300), target_class=None, layer_name=None):\n",
    "    \"\"\"\n",
    "    Generate a Grad-CAM heatmap + overlay for a given image and model.\n",
    "    - model: Keras model\n",
    "    - img_bgr: input image in BGR (OpenCV) format\n",
    "    - target_size: model input size (W, H). Use (300,300) if you trained at 300; (128,128) for your minor model.\n",
    "    - target_class: optional int for class index; if None, uses model's top prediction\n",
    "    - layer_name: optional conv layer name; if None, picks the last Conv2D layer\n",
    "    Returns: (heatmap_resized [H x W float 0..1], overlay_bgr [H x W x 3 uint8])\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    img = cv2.resize(img_bgr, target_size)\n",
    "    x = img.astype(\"float32\") / 255.0\n",
    "    x = np.expand_dims(x, 0)\n",
    "\n",
    "    # Pick a conv layer if not provided\n",
    "    if layer_name is None:\n",
    "        for l in reversed(model.layers):\n",
    "            if isinstance(l, tf.keras.layers.Conv2D):\n",
    "                layer_name = l.name\n",
    "                break\n",
    "        if layer_name is None:  # fallback (if no Conv2D found)\n",
    "            layer_name = model.layers[-3].name\n",
    "\n",
    "    # Build a model that maps the input image to the activations of the target layer\n",
    "    # and the final class predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, preds = grad_model(x)\n",
    "        if target_class is None:\n",
    "            target_class = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, target_class]\n",
    "\n",
    "    # Gradients of the target class w.r.t. the feature map\n",
    "    grads = tape.gradient(class_channel, conv_out)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_out = conv_out[0]  # [H', W', C]\n",
    "    heatmap = conv_out @ pooled_grads[..., tf.newaxis]  # [H', W', 1]\n",
    "    heatmap = tf.squeeze(heatmap)                       # [H', W']\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)\n",
    "    heatmap = heatmap.numpy()\n",
    "\n",
    "    # Resize to original image, colorize, and overlay\n",
    "    heatmap_resized = cv2.resize(heatmap, (img_bgr.shape[1], img_bgr.shape[0]))\n",
    "    heatmap_uint8 = np.uint8(255 * heatmap_resized)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(img_bgr, 0.6, heatmap_color, 0.4, 0)\n",
    "\n",
    "    return heatmap_resized, overlay\n",
    "\n",
    "def artifact_reasons(bgr, heatmap):\n",
    "    \"\"\"\n",
    "    Lightweight heuristics to produce short textual 'why' reasons.\n",
    "    Returns a list of up to 3 bullet-worthy strings.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 1) High-frequency artifacts\n",
    "    lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "    # 2) Edge halos / boundary inconsistencies\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    edge_mag = np.mean(np.sqrt(sobelx**2 + sobely**2))\n",
    "\n",
    "    # 3) Where Grad-CAM focuses (e.g., mouth/eyes area proxy)\n",
    "    h, w = heatmap.shape\n",
    "    eye_mouth_band = heatmap[int(0.25*h):int(0.7*h), int(0.2*w):int(0.8*w)]\n",
    "    focus_ratio = float(np.mean(eye_mouth_band > 0.5))\n",
    "\n",
    "    reasons = []\n",
    "    if lap_var > 2000:\n",
    "        reasons.append(\"Unnatural high-frequency noise patterns detected (compression or GAN artifacts).\")\n",
    "    if edge_mag > 25:\n",
    "        reasons.append(\"Edge halos/abrupt transitions around facial regions suggest blending or face swap seams.\")\n",
    "    if focus_ratio > 0.25:\n",
    "        reasons.append(\"Model attention peaks around eyes/mouth where lip-sync or blink artifacts commonly appear.\")\n",
    "    if not reasons:\n",
    "        reasons.append(\"Subtle inconsistencies in texture and lighting highlighted by the attention map.\")\n",
    "    return reasons[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8839085",
   "metadata": {},
   "source": [
    "### Load model and predict + explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051ac27",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nt.getcwd() takes no arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m----> 4\u001b[0m MODEL_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepfake_detector_keras.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m IMG_SIZE \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: nt.getcwd() takes no arguments (1 given)"
     ]
    }
   ],
   "source": [
    "import os, json, cv2, numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_DIR = os.path.join(os.getcwd(), \"models\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"deepfake_detector_keras.h5\")\n",
    "IMG_SIZE = (128,128)\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "def predict(img):\n",
    "    img = np.array(img)\n",
    "    bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    x = cv2.resize(img, IMG_SIZE).astype('float32')/255.0\n",
    "    x = np.expand_dims(x, 0)\n",
    "    prob = float(model.predict(x, verbose=0)[0][0])\n",
    "    label = \"Real\" if prob > 0.5 else \"Fake\"\n",
    "    conf = prob if label == \"Real\" else 1 - prob\n",
    "\n",
    "    # Grad-CAM & reasons\n",
    "    heatmap, overlay = grad_cam(model, bgr, target_size=IMG_SIZE, target_class=None)\n",
    "    reasons = artifact_reasons(bgr, heatmap)\n",
    "    return {\"Prediction\": label, \"Confidence\": round(conf, 4)}, overlay[:, :, ::-1], \"\\n\".join(\"• \" + r for r in reasons)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
